"""
Bug Classifier Service
Orchestrator service Ä‘á»ƒ phÃ¢n loáº¡i bug reports
Äiá»u phá»‘i giá»¯a GPT vÃ  Llama models
"""

import os
import asyncio
import logging
from typing import List, Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Táº¯t log HTTP requests tá»« httpx
logging.getLogger("httpx").setLevel(logging.WARNING)

# Enable debug logs for semantic search details
# Uncomment dÃ²ng dÆ°á»›i Ä‘á»ƒ tháº¥y chi tiáº¿t similarity scores:
# logger.setLevel(logging.DEBUG)

# Import configuration tá»« package config
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config import BUG_LABELS, TEAM_GROUPS, LABEL_TO_TEAM, FEW_SHOT_EXAMPLES

# Import model services
try:
    from services.gpt_service import get_gpt_service
    GPT_AVAILABLE = True
    logger.info("âœ… GPT service available")
except ImportError as e:
    GPT_AVAILABLE = False
    logger.warning(f"âš ï¸ GPT service not available: {e}")

try:
    from services.llama_service import get_llama_service
    LLAMA_AVAILABLE = True
    logger.info("âœ… Llama service available")
    # Pre-initialize singleton Ä‘á»ƒ trÃ¡nh load model má»—i request
    try:
        _llama_singleton = get_llama_service()
        logger.info("âœ… Llama singleton pre-loaded")
    except Exception as e:
        logger.warning(f"âš ï¸ Could not pre-load Llama model: {e}")
except ImportError as e:
    LLAMA_AVAILABLE = False
    logger.warning(f"âš ï¸ Llama service not available: {e}")
except Exception as e:
    LLAMA_AVAILABLE = False
    logger.error(f"âŒ Error importing Llama service: {e}")

# Vector store (previously ChromaDB) - now backed by Pinecone via models/vector_store
try:
    from models.vector_store import (
        search_similar_classified_bugs,
        get_dynamic_few_shot_examples,
        add_classified_bug_to_vector_store,
        get_vector_store_stats
    )
    VECTOR_STORE_AVAILABLE = True
    logger.info("âœ… Vector store available (Pinecone-backed)")
except ImportError as e:
    VECTOR_STORE_AVAILABLE = False
    logger.warning(f"âš ï¸ Vector store not available: {e}")


# Helper functions
def _label_line(label, v):
    kws = v.get("keywords") or []
    kw_text = f" (keywords: {', '.join(kws)})" if kws else ""
    return f"- {label}: {v.get('desc', '')}{kw_text}"

label_descriptions = "\n".join(
    [_label_line(label, v) for label, v in BUG_LABELS.items()]
)

# Build example text for few-shot learning
example_text = "\n".join(
    [
        f"Bug report: \"{ex['description']}\"\nPhÃ¢n loáº¡i: {ex['label']}"
        for ex in FEW_SHOT_EXAMPLES
    ]
)

def _quick_heuristic_for_text(description: str):
    """PhÃ¢n loáº¡i nhanh báº±ng keyword matching - yÃªu cáº§u >60% tá»« trong cÃ¢u match vá»›i keywords"""
    import re
    
    desc_lower = (description or "").lower()
    
    # TÃ¡ch cÃ¡c tá»« trong description (bá» kÃ½ tá»± Ä‘áº·c biá»‡t)
    desc_words = re.findall(r'\b\w+\b', desc_lower)
    total_words = len(desc_words)
    
    if total_words == 0:
        return None
    
    keyword_scores = {}
    keyword_matches = {}
    match_percentages = {}

    for label, v in BUG_LABELS.items():
        kws = v.get("keywords") or []
        matched_words = set()
        matches = []
        
        for kw in kws:
            if not kw:
                continue
            kw_lower = kw.lower()
            # Match whole word
            pattern = r'\b' + re.escape(kw_lower) + r'\b'
            if re.search(pattern, desc_lower):
                matches.append(kw)
                # Äáº¿m cÃ¡c tá»« trong keyword Ä‘Æ°á»£c match
                kw_words = re.findall(r'\b\w+\b', kw_lower)
                matched_words.update(kw_words)
        
        # TÃ­nh % tá»« trong description Ä‘Æ°á»£c match bá»Ÿi keywords
        matched_desc_words = sum(1 for word in desc_words if word in matched_words)
        match_percentage = (matched_desc_words / total_words) * 100
        
        keyword_scores[label] = len(matches)
        match_percentages[label] = match_percentage
        if matches:
            keyword_matches[label] = matches

    # TÃ¬m label cÃ³ % match cao nháº¥t vÃ  > 60%
    if match_percentages:
        best_label = max(match_percentages, key=lambda k: match_percentages[k])
        best_percentage = match_percentages[best_label]
        
        if best_percentage > 60:
            team = LABEL_TO_TEAM.get(best_label)
            return {
                "label": best_label,
                "reason": f"Matched {best_percentage:.0f}% keywords: {', '.join(keyword_matches.get(best_label, []))} (heuristic)",
                "team": team,
                }
    return None


async def classify_bug(description: str, model: str = "GPT-5"):
    """
    PhÃ¢n loáº¡i bug report vá»›i multi-layer approach:
    1. Keyword heuristic (nhanh nháº¥t)
    2. Semantic search tá»« vector store (cached bugs)
    3. LLM classification vá»›i dynamic few-shot examples
    
    Args:
        description: MÃ´ táº£ bug
        model: "Llama" hoáº·c "GPT-5"
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"ðŸ” CLASSIFY_BUG - Model: {model}")
    logger.info(f"ðŸ“ Input: {description[:100]}..." if len(description) > 100 else f"ðŸ“ Input: {description}")
    
    # BÆ°á»›c 1: Thá»­ keyword heuristic (nhanh nháº¥t)
    heuristic_result = _quick_heuristic_for_text(description)
    if heuristic_result:
        logger.info(f"âš¡ Heuristic match: {heuristic_result}")
        # KhÃ´ng lÆ°u vÃ o vector store - quÃ¡ rÃµ rÃ ng, chá»‰ dá»±a vÃ o keywords
        return heuristic_result
    
    # XÃ¡c Ä‘á»‹nh embedding type dá»±a trÃªn model
    use_local = (model == "Llama")
    
    # Track if high similarity match exists (>= 85%)
    has_high_similarity_match = False
    
    # BÆ°á»›c 2: Semantic search trong vector store (bugs Ä‘Ã£ classify)
    if VECTOR_STORE_AVAILABLE:
        try:
            similar_bugs = search_similar_classified_bugs(
                query=description,
                top_k=1,
                similarity_threshold=0.85,  # High similarity threshold (85%)
                use_local_embeddings=use_local
            )
            
            if similar_bugs and len(similar_bugs) > 0:
                best_match = similar_bugs[0]
                similarity = best_match.get('similarity', 0)
                metadata = best_match.get('metadata', {})
                
                if similarity >= 0.85:
                    has_high_similarity_match = True  # Mark that we found high similarity
                    
                    if metadata.get('label'):  # Very similar bug found with label
                        result = {
                            'label': metadata.get('label'),
                            'reason': f"Similar to: '{best_match.get('text', '')[:60]}...' (semantic: {similarity:.0%})",
                            'team': metadata.get('team'),
                            'severity': metadata.get('severity')
                        }
                        logger.info(f"ðŸŽ¯ Semantic match: {result}")
                        # KhÃ´ng lÆ°u vÃ o vector store - Ä‘Ã£ cÃ³ bug tÆ°Æ¡ng tá»± trong DB
                        return result
        except Exception as e:
            logger.warning(f"âš ï¸ Vector store search failed: {e}")
    
    # BÆ°á»›c 3: Get dynamic few-shot examples tá»« vector store
    dynamic_examples = FEW_SHOT_EXAMPLES  # Default
    if VECTOR_STORE_AVAILABLE:
        try:
            retrieved_examples = get_dynamic_few_shot_examples(
                description,
                top_k=5,
                use_local_embeddings=use_local
            )
            if retrieved_examples:
                # Convert to format compatible vá»›i existing code
                dynamic_examples = [
                    {'description': ex['description'], 'label': ex['label']}
                    for ex in retrieved_examples
                ]
                logger.info(f"âœ… Using {len(dynamic_examples)} dynamic examples")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to get dynamic examples: {e}")
    
    # BÆ°á»›c 4: LLM classification theo model Ä‘Æ°á»£c chá»n
    if model == "Llama":
        # Xá»­ lÃ½ LLAMA
        if not LLAMA_AVAILABLE:
            logger.error(f"âŒ Llama khÃ´ng kháº£ dá»¥ng")
            return {"label": "", "reason": "Llama model not available", "team": None, "severity": None}
        
        try:
            logger.info("ðŸ¦™ Äang xá»­ lÃ½ báº±ng Llama...")
            llama_service = get_llama_service()
            result = await asyncio.to_thread(
                llama_service.classify_bug,
                description,
                list(BUG_LABELS.keys()),
                dynamic_examples  # Use dynamic examples
            )
            # Map team
            if not result.get('team') and result.get('label'):
                result['team'] = LABEL_TO_TEAM.get(result['label'])
            logger.info(f"âœ… Llama result: {result}")
            
            # LÆ°u vÃ o vector store náº¿u khÃ´ng cÃ³ match >= 85%
            if VECTOR_STORE_AVAILABLE and result.get('label') and not has_high_similarity_match:
                try:
                    add_classified_bug_to_vector_store(
                        bug_text=description,
                        label=result['label'],
                        reason=result.get('reason', ''),
                        team=result.get('team'),
                        severity=result.get('severity'),
                        use_local_embeddings=True  # Local embeddings cho Llama
                    )
                    logger.info("ðŸ’¾ Saved to vector store (LOCAL embeddings)")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to save to vector store: {e}")
            elif has_high_similarity_match:
                logger.info("â­ï¸  Skipped saving - high similarity match exists (>= 85%)")
            
            return result
        except Exception as e:
            logger.error(f"âŒ Llama lá»—i: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"label": "", "reason": f"Llama error: {str(e)}", "team": None, "severity": None}
    
    elif model == "GPT-5":
        # Xá»­ lÃ½ GPT
        if not GPT_AVAILABLE:
            logger.error("âŒ GPT khÃ´ng kháº£ dá»¥ng")
            return {"label": "", "reason": "GPT model not available", "team": None, "severity": None}
        
        try:
            logger.info("ðŸ¤– Äang xá»­ lÃ½ báº±ng GPT...")
            gpt_service = get_gpt_service()
            result = await gpt_service.classify_bug(
                description=description,
                labels=list(BUG_LABELS.keys()),
                label_descriptions=label_descriptions,
                example_text=example_text,
                team_groups=list(TEAM_GROUPS.keys())
            )
            # Map team
            if not result.get('team') and result.get('label'):
                result['team'] = LABEL_TO_TEAM.get(result['label'])
            logger.info(f"âœ… GPT result: {result}")
            
            # LÆ°u vÃ o vector store náº¿u khÃ´ng cÃ³ match >= 85%
            if VECTOR_STORE_AVAILABLE and result.get('label') and not has_high_similarity_match:
                try:
                    add_classified_bug_to_vector_store(
                        bug_text=description,
                        label=result['label'],
                        reason=result.get('reason', ''),
                        team=result.get('team'),
                        severity=result.get('severity'),
                        use_local_embeddings=False  # OpenAI embeddings cho GPT
                    )
                    logger.info("ðŸ’¾ Saved to vector store (OPENAI embeddings)")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to save to vector store: {e}")
            elif has_high_similarity_match:
                logger.info("â­ï¸  Skipped saving - high similarity match exists (>= 85%)")
            
            return result
        except Exception as e:
            logger.error(f"âŒ GPT lá»—i: {e}")
            return {"label": "", "reason": f"GPT error: {str(e)}", "team": None, "severity": None}
    
    else:
        # Model khÃ´ng há»— trá»£
        logger.error(f"âŒ Model '{model}' khÃ´ng Ä‘Æ°á»£c há»— trá»£")
        return {"label": "", "reason": f"Unsupported model: {model}", "team": None, "severity": None}


async def batch_classify(descriptions: List[str], model: str = "GPT-5"):
    logger.info(f"\n{'='*80}")
    logger.info(f"ðŸ“¦ BATCH_CLASSIFY - Model: {model}, Count: {len(descriptions)}")
    results: List[Optional[dict]] = [None] * len(descriptions)

    # XÃ¡c Ä‘á»‹nh embedding type
    use_local = (model == "Llama")
    
    # TIER 1: Heuristic pass (keyword matching >60%)
    remaining_indexes = []
    for i, desc in enumerate(descriptions):
        h = _quick_heuristic_for_text(desc)
        if h:
            results[i] = h
        else:
            remaining_indexes.append(i)
    
    logger.info(f"âš¡ Tier 1 Heuristic: {len(descriptions) - len(remaining_indexes)}/{len(descriptions)} matched")

    if not remaining_indexes:
        # Heuristic match 100% â†’ KhÃ´ng cáº§n lÆ°u vÃ o vector store (quÃ¡ rÃµ rÃ ng)
        return results
    
    # TIER 2: Vector store Semantic Search (>85% similarity)
    # Track which bugs have high similarity matches (>= 85%)
    high_similarity_bugs = set()  # Set of bug indexes with >= 85% similarity
    semantic_remaining = []
    if VECTOR_STORE_AVAILABLE:
        logger.info(f"ðŸ” Tier 2 Semantic Search: Checking {len(remaining_indexes)} bugs...")
        for idx in remaining_indexes:
            try:
                query_text = descriptions[idx]
                logger.debug(f"Searching for bug {idx}: {query_text[:100]}...")
                
                similar_bugs = search_similar_classified_bugs(
                    query=query_text,
                    top_k=3,  # Láº¥y 3 káº¿t quáº£ Ä‘á»ƒ debug
                    similarity_threshold=0.0,  # Táº¯t threshold Ä‘á»ƒ tháº¥y táº¥t cáº£
                    use_local_embeddings=use_local
                )
                
                if similar_bugs and len(similar_bugs) > 0:
                    best_match = similar_bugs[0]
                    similarity = best_match.get('similarity', 0)
                    
                    # Ensure similarity is a number
                    try:
                        similarity = float(similarity) if similarity is not None else 0
                    except (ValueError, TypeError):
                        logger.warning(f"   Bug {idx}: Invalid similarity value: {similarity} (type: {type(similarity)})")
                        similarity = 0
                    
                    metadata = best_match.get('metadata', {})
                    
                    # Debug: In ra top 3 matches
                    if len(similar_bugs) > 1:
                        logger.info(f"   Bug {idx} top matches: " + ", ".join([f"{s.get('similarity', 0):.1%}" for s in similar_bugs[:3]]))
                    
                    # Debug similarity value and type
                    logger.debug(f"   Bug {idx}: similarity = {similarity}, type = {type(similarity)}, >= 0.85? {similarity >= 0.85 if similarity is not None else 'None'}")
                    
                    if similarity >= 0.85:
                        high_similarity_bugs.add(idx)  # Mark as high similarity
                        
                        if metadata.get('label'):
                            results[idx] = {
                                'label': metadata.get('label'),
                                'reason': f"Similar: '{best_match.get('text', '')[:40]}...' ({similarity:.0%})",
                                'team': metadata.get('team'),
                                'severity': metadata.get('severity')
                            }
                            continue
                        else:
                            # Debug: why no label?
                            logger.info(f"   Bug {idx}: High similarity ({similarity:.1%}) but metadata missing 'label' field")
                            logger.debug(f"      metadata = {metadata}")
                            logger.debug(f"      best_match = {best_match}")
                    else:
                        logger.info(f"   Bug {idx}: Found similar but only {similarity:.1%} < 85% threshold")
            except Exception as e:
                logger.error(f"âŒ Semantic search failed for bug {idx}: {e}", exc_info=True)
            
            semantic_remaining.append(idx)
        
        semantic_matched = len(remaining_indexes) - len(semantic_remaining)
        logger.info(f"âœ… Tier 2 Semantic: {semantic_matched}/{len(remaining_indexes)} matched")
        remaining_indexes = semantic_remaining
    
    if not remaining_indexes:
        # Semantic match tá»« vector store â†’ KhÃ´ng cáº§n lÆ°u láº¡i (Ä‘Ã£ cÃ³ trong DB)
        return results
    
    # TIER 3 & 4: LLM Classification vá»›i dynamic few-shot examples
    logger.info(f"ðŸ¤– Tier 3+4 LLM: Processing {len(remaining_indexes)} bugs with {model}...")
    if model == "Llama":
        # Xá»­ lÃ½ LLAMA batch
        if not LLAMA_AVAILABLE:
            logger.error(f"âŒ Llama khÃ´ng kháº£ dá»¥ng")
            for idx in remaining_indexes:
                results[idx] = {"label": "", "reason": "Llama model not available", "team": None, "severity": None}
            return results
        
        logger.info("ðŸ¦™ Äang xá»­ lÃ½ batch báº±ng Llama...")
        llama_service = get_llama_service()
        
        # Get descriptions for remaining bugs
        remaining_descriptions = [descriptions[idx] for idx in remaining_indexes]
        
        # Try batch classification first (more efficient)
        try:
            batch_results = await asyncio.to_thread(
                llama_service.batch_classify_bugs,
                remaining_descriptions,
                list(BUG_LABELS.keys()),
                FEW_SHOT_EXAMPLES
            )
            
            # Map results back
            for i, idx in enumerate(remaining_indexes):
                if i < len(batch_results):
                    result = batch_results[i]
                    # Add team mapping
                    if not result.get('team') and result.get('label'):
                        result['team'] = LABEL_TO_TEAM.get(result['label'])
                    results[idx] = result
            
            logger.info(f"âœ… Llama batch classification complete")
        except Exception as e:
            logger.error(f"âŒ Llama batch error: {e}, falling back to individual classification")
            # Fallback: classify individually
            for idx in remaining_indexes:
                try:
                    result = await classify_bug(descriptions[idx], model="Llama")
                    results[idx] = result
                except Exception as e2:
                    logger.error(f"âŒ Llama lá»—i bug {idx}: {e2}")
                    results[idx] = {"label": "", "reason": f"Llama error: {str(e2)}", "team": None, "severity": None}
        
    
    elif model == "GPT-5":
        # Xá»­ lÃ½ GPT batch
        if not GPT_AVAILABLE:
            logger.error("âŒ GPT khÃ´ng kháº£ dá»¥ng")
            for idx in remaining_indexes:
                results[idx] = {"label": "", "reason": "GPT model not available", "team": None, "severity": None}
            return results
        
        logger.info("ðŸ¤– Äang xá»­ lÃ½ batch báº±ng GPT...")
        gpt_service = get_gpt_service()
        
        # Láº¥y descriptions vÃ  indexes cÃ²n láº¡i
        remaining_descriptions = [descriptions[idx] for idx in remaining_indexes]
        
        # Gá»i GPT batch API
        batch_results = await gpt_service.batch_classify(
            descriptions=remaining_descriptions,
            indexes=remaining_indexes,
            labels=list(BUG_LABELS.keys()),
            label_descriptions=label_descriptions,
            example_text=example_text,
            team_groups=list(TEAM_GROUPS.keys())
        )
        
        # Map káº¿t quáº£ vá»
        for idx, result in batch_results.items():
            if 0 <= idx < len(results):
                if not result.get('team') and result.get('label'):
                    result['team'] = LABEL_TO_TEAM.get(result['label'])
                results[idx] = result
    
    else:
        # Model khÃ´ng há»— trá»£
        logger.error(f"âŒ Model '{model}' khÃ´ng Ä‘Æ°á»£c há»— trá»£")
        for idx in remaining_indexes:
            results[idx] = {"label": "", "reason": f"Unsupported model: {model}", "team": None, "severity": None}
        return results

    # Fallback individual classification for None entries
    none_count = sum(1 for r in results if r is None)
    if none_count > 0:
        logger.info(f"ðŸ”„ Fallback individual classification for {none_count} bugs")
    
    for i in range(len(results)):
        if results[i] is None:
            try:
                results[i] = await classify_bug(descriptions[i], model=model)
            except Exception as e:
                logger.error(f"âŒ Failed to classify bug {i}: {e}")
                results[i] = {
                    "label": "",
                    "reason": "classification_failed",
                    "team": None,
                }
    
    # TIER 5: LÆ°u káº¿t quáº£ vÃ o vector store (bá» qua nhá»¯ng bug cÃ³ high similarity >= 85%)
    if VECTOR_STORE_AVAILABLE:
        saved_count = 0
        skipped_count = 0
        for i, result in enumerate(results):
            if result and result.get('label'):
                # Skip saving if bug has high similarity match (>= 85%)
                if i in high_similarity_bugs:
                    skipped_count += 1
                    logger.debug(f"Skipped bug {i} - high similarity match exists")
                    continue
                    
                try:
                    add_classified_bug_to_vector_store(
                        bug_text=descriptions[i],
                        label=result['label'],
                        reason=result.get('reason', ''),
                        team=result.get('team'),
                        severity=result.get('severity'),
                        use_local_embeddings=use_local
                    )
                    saved_count += 1
                except Exception as e:
                    logger.debug(f"Failed to save bug {i}: {e}")
        
        logger.info(f"ðŸ’¾ Saved {saved_count}/{len(results)} results to vector store (skipped {skipped_count} with high similarity)")
    
    logger.info(f"âœ… Batch classification complete: {len(results)} results")
    logger.info(f"{'='*80}\n")
    return results


# CLI interface khi cháº¡y trá»±c tiáº¿p
if __name__ == "__main__":
    bug_report = input("Nháº­p ná»™i dung bug report: ")

    try:
        res = asyncio.run(classify_bug(bug_report))
    except Exception as e:
        print(f"Classification error: {e}")
        res = None

    if isinstance(res, dict):
        print(
            f"\nBug report: {bug_report}\nPhÃ¢n loáº¡i: {res.get('label')}\nLÃ½ do: {res.get('reason')}"
        )
    else:
        print(f"\nBug report: {bug_report}\nPhÃ¢n loáº¡i: {res}")
    input(".")
