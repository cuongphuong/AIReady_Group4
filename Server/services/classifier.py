"""
Bug Classifier Service
Orchestrator service ƒë·ªÉ ph√¢n lo·∫°i bug reports
ƒêi·ªÅu ph·ªëi gi·ªØa GPT v√† Llama models
"""

import os
import asyncio
import logging
from typing import List, Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import configuration t·ª´ package config
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config import BUG_LABELS, TEAM_GROUPS, LABEL_TO_TEAM, FEW_SHOT_EXAMPLES

# Import model services
try:
    from services.gpt_service import get_gpt_service
    GPT_AVAILABLE = True
    logger.info("‚úÖ GPT service available")
except ImportError as e:
    GPT_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è GPT service not available: {e}")

try:
    from services.llama_service import get_llama_service
    LLAMA_AVAILABLE = True
    logger.info("‚úÖ Llama service available")
    # Pre-initialize singleton ƒë·ªÉ tr√°nh load model m·ªói request
    try:
        _llama_singleton = get_llama_service()
        logger.info("‚úÖ Llama singleton pre-loaded")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not pre-load Llama model: {e}")
except ImportError as e:
    LLAMA_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Llama service not available: {e}")
except Exception as e:
    LLAMA_AVAILABLE = False
    logger.error(f"‚ùå Error importing Llama service: {e}")


# Helper functions
def _label_line(label, v):
    kws = v.get("keywords") or []
    kw_text = f" (keywords: {', '.join(kws)})" if kws else ""
    return f"- {label}: {v.get('desc', '')}{kw_text}"

label_descriptions = "\n".join(
    [_label_line(label, v) for label, v in BUG_LABELS.items()]
)

# Build example text for few-shot learning
example_text = "\n".join(
    [
        f"Bug report: \"{ex['description']}\"\nPh√¢n lo·∫°i: {ex['label']}"
        for ex in FEW_SHOT_EXAMPLES
    ]
)

def _quick_heuristic_for_text(description: str):
    """Ph√¢n lo·∫°i nhanh b·∫±ng keyword matching (whole word only)"""
    import re
    
    desc_lower = (description or "").lower()
    keyword_scores = {}
    keyword_matches = {}

    for label, v in BUG_LABELS.items():
        kws = v.get("keywords") or []
        score = 0
        matches = []
        for kw in kws:
            if not kw:
                continue
            # Ch·ªâ match whole word ƒë·ªÉ tr√°nh false positive (VD: "load" trong "Download")
            # Use word boundary \b to match complete words only
            pattern = r'\b' + re.escape(kw.lower()) + r'\b'
            if re.search(pattern, desc_lower):
                score += 1
                matches.append(kw)
        keyword_scores[label] = score
        if matches:
            keyword_matches[label] = matches

    if keyword_scores:
        best_label = max(keyword_scores, key=lambda k: keyword_scores[k])
        # Y√™u c·∫ßu √≠t nh·∫•t 2 keywords match ƒë·ªÉ tin t∆∞·ªüng h∆°n (ho·∫∑c 1 keyword n·∫øu match duy nh·∫•t)
        if keyword_scores[best_label] >= 2:
            top_scores = [
                s for s in keyword_scores.values() if s == keyword_scores[best_label]
            ]
            if len(top_scores) == 1:
                team = LABEL_TO_TEAM.get(best_label)
                return {
                    "label": best_label,
                    "reason": f"Matched keywords: {', '.join(keyword_matches.get(best_label, []))} (heuristic)",
                    "team": team,
                }
        # N·∫øu ch·ªâ c√≥ 1 keyword match v√† kh√¥ng c√≥ label n√†o kh√°c match, ch·∫•p nh·∫≠n
        elif keyword_scores[best_label] == 1:
            total_matches = sum(1 for s in keyword_scores.values() if s > 0)
            if total_matches == 1:  # Ch·ªâ c√≥ 1 label match duy nh·∫•t
                team = LABEL_TO_TEAM.get(best_label)
                return {
                    "label": best_label,
                    "reason": f"Matched keyword: {', '.join(keyword_matches.get(best_label, []))} (heuristic)",
                    "team": team,
                }
    return None


async def classify_bug(description: str, model: str = "GPT-5"):
    """
    Ph√¢n lo·∫°i bug report
    
    Args:
        description: M√¥ t·∫£ bug
        model: "Llama" ho·∫∑c "GPT-5"
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"üîç CLASSIFY_BUG - Model: {model}")
    logger.info(f"üìù Input: {description[:100]}..." if len(description) > 100 else f"üìù Input: {description}")
    
    # B∆∞·ªõc 1: Th·ª≠ heuristic matching (nhanh nh·∫•t)
    heuristic_result = _quick_heuristic_for_text(description)
    if heuristic_result:
        logger.info(f"‚ö° Heuristic match: {heuristic_result}")
        return heuristic_result
    
    # B∆∞·ªõc 2: X·ª≠ l√Ω theo model ƒë∆∞·ª£c ch·ªçn
    if model == "Llama":
        # X·ª≠ l√Ω LLAMA
        if not LLAMA_AVAILABLE:
            logger.error(f"‚ùå Llama kh√¥ng kh·∫£ d·ª•ng")
            return {"label": "", "reason": "Llama model not available", "team": None, "severity": None}
        
        try:
            logger.info("ü¶ô ƒêang x·ª≠ l√Ω b·∫±ng Llama...")
            llama_service = get_llama_service()
            result = await asyncio.to_thread(
                llama_service.classify_bug,
                description,
                list(BUG_LABELS.keys()),
                FEW_SHOT_EXAMPLES
            )
            # Map team
            if not result.get('team') and result.get('label'):
                result['team'] = LABEL_TO_TEAM.get(result['label'])
            logger.info(f"‚úÖ Llama result: {result}")
            return result
        except Exception as e:
            logger.error(f"‚ùå Llama l·ªói: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"label": "", "reason": f"Llama error: {str(e)}", "team": None, "severity": None}
    
    elif model == "GPT-5":
        # X·ª≠ l√Ω GPT
        if not GPT_AVAILABLE:
            logger.error("‚ùå GPT kh√¥ng kh·∫£ d·ª•ng")
            return {"label": "", "reason": "GPT model not available", "team": None, "severity": None}
        
        try:
            logger.info("ü§ñ ƒêang x·ª≠ l√Ω b·∫±ng GPT...")
            gpt_service = get_gpt_service()
            result = await gpt_service.classify_bug(
                description=description,
                labels=list(BUG_LABELS.keys()),
                label_descriptions=label_descriptions,
                example_text=example_text,
                team_groups=list(TEAM_GROUPS.keys())
            )
            # Map team
            if not result.get('team') and result.get('label'):
                result['team'] = LABEL_TO_TEAM.get(result['label'])
            logger.info(f"‚úÖ GPT result: {result}")
            return result
        except Exception as e:
            logger.error(f"‚ùå GPT l·ªói: {e}")
            return {"label": "", "reason": f"GPT error: {str(e)}", "team": None, "severity": None}
    
    else:
        # Model kh√¥ng h·ªó tr·ª£
        logger.error(f"‚ùå Model '{model}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£")
        return {"label": "", "reason": f"Unsupported model: {model}", "team": None, "severity": None}


async def batch_classify(descriptions: List[str], model: str = "GPT-5"):
    logger.info(f"\n{'='*80}")
    logger.info(f"üì¶ BATCH_CLASSIFY - Model: {model}, Count: {len(descriptions)}")
    results: List[Optional[dict]] = [None] * len(descriptions)

    # Heuristic pass
    remaining_indexes = []
    for i, desc in enumerate(descriptions):
        h = _quick_heuristic_for_text(desc)
        if h:
            results[i] = h
        else:
            remaining_indexes.append(i)
    
    logger.info(f"‚ö° Heuristic matched: {len(descriptions) - len(remaining_indexes)}/{len(descriptions)}")
    logger.info(f"üîÑ Remaining for model: {len(remaining_indexes)}")

    if not remaining_indexes:
        return results
    
    # X·ª≠ l√Ω theo model ƒë∆∞·ª£c ch·ªçn
    if model == "Llama":
        # X·ª≠ l√Ω LLAMA batch
        if not LLAMA_AVAILABLE:
            logger.error(f"‚ùå Llama kh√¥ng kh·∫£ d·ª•ng")
            for idx in remaining_indexes:
                results[idx] = {"label": "", "reason": "Llama model not available", "team": None, "severity": None}
            return results
        
        logger.info("ü¶ô ƒêang x·ª≠ l√Ω batch b·∫±ng Llama...")
        llama_service = get_llama_service()
        
        # Get descriptions for remaining bugs
        remaining_descriptions = [descriptions[idx] for idx in remaining_indexes]
        
        # Try batch classification first (more efficient)
        try:
            batch_results = await asyncio.to_thread(
                llama_service.batch_classify_bugs,
                remaining_descriptions,
                list(BUG_LABELS.keys()),
                FEW_SHOT_EXAMPLES
            )
            
            # Map results back
            for i, idx in enumerate(remaining_indexes):
                if i < len(batch_results):
                    result = batch_results[i]
                    # Add team mapping
                    if not result.get('team') and result.get('label'):
                        result['team'] = LABEL_TO_TEAM.get(result['label'])
                    results[idx] = result
            
            logger.info(f"‚úÖ Llama batch classification complete")
        except Exception as e:
            logger.error(f"‚ùå Llama batch error: {e}, falling back to individual classification")
            # Fallback: classify individually
            for idx in remaining_indexes:
                try:
                    result = await classify_bug(descriptions[idx], model="Llama")
                    results[idx] = result
                except Exception as e2:
                    logger.error(f"‚ùå Llama l·ªói bug {idx}: {e2}")
                    results[idx] = {"label": "", "reason": f"Llama error: {str(e2)}", "team": None, "severity": None}
        
        logger.info(f"‚úÖ Batch classification complete: {len(results)} results")
        return results
    
    elif model == "GPT-5":
        # X·ª≠ l√Ω GPT batch
        if not GPT_AVAILABLE:
            logger.error("‚ùå GPT kh√¥ng kh·∫£ d·ª•ng")
            for idx in remaining_indexes:
                results[idx] = {"label": "", "reason": "GPT model not available", "team": None, "severity": None}
            return results
        
        logger.info("ü§ñ ƒêang x·ª≠ l√Ω batch b·∫±ng GPT...")
        gpt_service = get_gpt_service()
        
        # L·∫•y descriptions v√† indexes c√≤n l·∫°i
        remaining_descriptions = [descriptions[idx] for idx in remaining_indexes]
        
        # G·ªçi GPT batch API
        batch_results = await gpt_service.batch_classify(
            descriptions=remaining_descriptions,
            indexes=remaining_indexes,
            labels=list(BUG_LABELS.keys()),
            label_descriptions=label_descriptions,
            example_text=example_text,
            team_groups=list(TEAM_GROUPS.keys())
        )
        
        # Map k·∫øt qu·∫£ v·ªÅ
        for idx, result in batch_results.items():
            if 0 <= idx < len(results):
                if not result.get('team') and result.get('label'):
                    result['team'] = LABEL_TO_TEAM.get(result['label'])
                results[idx] = result
    
    else:
        # Model kh√¥ng h·ªó tr·ª£
        logger.error(f"‚ùå Model '{model}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£")
        for idx in remaining_indexes:
            results[idx] = {"label": "", "reason": f"Unsupported model: {model}", "team": None, "severity": None}
        return results

    # Fallback individual classification for None entries
    none_count = sum(1 for r in results if r is None)
    if none_count > 0:
        logger.info(f"üîÑ Fallback individual classification for {none_count} bugs")
    
    for i in range(len(results)):
        if results[i] is None:
            try:
                results[i] = await classify_bug(descriptions[i], model=model)
            except Exception as e:
                logger.error(f"‚ùå Failed to classify bug {i}: {e}")
                results[i] = {
                    "label": "",
                    "reason": "classification_failed",
                    "team": None,
                }
    
    logger.info(f"‚úÖ Batch classification complete: {len(results)} results")
    return results


# CLI interface khi ch·∫°y tr·ª±c ti·∫øp
if __name__ == "__main__":
    bug_report = input("Nh·∫≠p n·ªôi dung bug report: ")

    try:
        res = asyncio.run(classify_bug(bug_report))
    except Exception as e:
        print(f"Classification error: {e}")
        res = None

    if isinstance(res, dict):
        print(
            f"\nBug report: {bug_report}\nPh√¢n lo·∫°i: {res.get('label')}\nL√Ω do: {res.get('reason')}"
        )
    else:
        print(f"\nBug report: {bug_report}\nPh√¢n lo·∫°i: {res}")
    input(".")
