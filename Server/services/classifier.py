"""
Bug Classifier Service
Orchestrator service ƒë·ªÉ ph√¢n lo·∫°i bug reports
ƒêi·ªÅu ph·ªëi gi·ªØa GPT v√† Llama models
"""

import os
import asyncio
import logging
from typing import List, Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# T·∫Øt log HTTP requests t·ª´ httpx
logging.getLogger("httpx").setLevel(logging.WARNING)

# Enable debug logs for semantic search details
# Uncomment d√≤ng d∆∞·ªõi ƒë·ªÉ th·∫•y chi ti·∫øt similarity scores:
# logger.setLevel(logging.DEBUG)

# Import configuration t·ª´ package config
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config import BUG_LABELS, TEAM_GROUPS, LABEL_TO_TEAM, FEW_SHOT_EXAMPLES

# Import model services
try:
    from services.gpt_service import get_gpt_service
    GPT_AVAILABLE = True
    logger.info("‚úÖ GPT service available")
except ImportError as e:
    GPT_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è GPT service not available: {e}")

try:
    from services.llama_service import get_llama_service
    LLAMA_AVAILABLE = True
    logger.info("‚úÖ Llama service available")
    # Pre-initialize singleton ƒë·ªÉ tr√°nh load model m·ªói request
    try:
        _llama_singleton = get_llama_service()
        logger.info("‚úÖ Llama singleton pre-loaded")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not pre-load Llama model: {e}")
except ImportError as e:
    LLAMA_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Llama service not available: {e}")
except Exception as e:
    LLAMA_AVAILABLE = False
    logger.error(f"‚ùå Error importing Llama service: {e}")

# Import ChromaDB vector store
try:
    from models.vector_store import (
        search_similar_classified_bugs,
        get_dynamic_few_shot_examples,
        add_classified_bug_to_vector_store,
        get_vector_store_stats
    )
    CHROMA_AVAILABLE = True
    logger.info("‚úÖ ChromaDB vector store available")
except ImportError as e:
    CHROMA_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è ChromaDB not available: {e}")


# Helper functions
def _label_line(label, v):
    kws = v.get("keywords") or []
    kw_text = f" (keywords: {', '.join(kws)})" if kws else ""
    return f"- {label}: {v.get('desc', '')}{kw_text}"

label_descriptions = "\n".join(
    [_label_line(label, v) for label, v in BUG_LABELS.items()]
)

# Build example text for few-shot learning
example_text = "\n".join(
    [
        f"Bug report: \"{ex['description']}\"\nPh√¢n lo·∫°i: {ex['label']}"
        for ex in FEW_SHOT_EXAMPLES
    ]
)

def _quick_heuristic_for_text(description: str):
    """Ph√¢n lo·∫°i nhanh b·∫±ng keyword matching - y√™u c·∫ßu >60% t·ª´ trong c√¢u match v·ªõi keywords"""
    import re
    
    desc_lower = (description or "").lower()
    
    # T√°ch c√°c t·ª´ trong description (b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát)
    desc_words = re.findall(r'\b\w+\b', desc_lower)
    total_words = len(desc_words)
    
    if total_words == 0:
        return None
    
    keyword_scores = {}
    keyword_matches = {}
    match_percentages = {}

    for label, v in BUG_LABELS.items():
        kws = v.get("keywords") or []
        matched_words = set()
        matches = []
        
        for kw in kws:
            if not kw:
                continue
            kw_lower = kw.lower()
            # Match whole word
            pattern = r'\b' + re.escape(kw_lower) + r'\b'
            if re.search(pattern, desc_lower):
                matches.append(kw)
                # ƒê·∫øm c√°c t·ª´ trong keyword ƒë∆∞·ª£c match
                kw_words = re.findall(r'\b\w+\b', kw_lower)
                matched_words.update(kw_words)
        
        # T√≠nh % t·ª´ trong description ƒë∆∞·ª£c match b·ªüi keywords
        matched_desc_words = sum(1 for word in desc_words if word in matched_words)
        match_percentage = (matched_desc_words / total_words) * 100
        
        keyword_scores[label] = len(matches)
        match_percentages[label] = match_percentage
        if matches:
            keyword_matches[label] = matches

    # T√¨m label c√≥ % match cao nh·∫•t v√† > 60%
    if match_percentages:
        best_label = max(match_percentages, key=lambda k: match_percentages[k])
        best_percentage = match_percentages[best_label]
        
        if best_percentage > 60:
            team = LABEL_TO_TEAM.get(best_label)
            return {
                "label": best_label,
                "reason": f"Matched {best_percentage:.0f}% keywords: {', '.join(keyword_matches.get(best_label, []))} (heuristic)",
                "team": team,
                }
    return None


async def classify_bug(description: str, model: str = "GPT-5"):
    """
    Ph√¢n lo·∫°i bug report v·ªõi multi-layer approach:
    1. Keyword heuristic (nhanh nh·∫•t)
    2. Semantic search t·ª´ ChromaDB (cached bugs)
    3. LLM classification v·ªõi dynamic few-shot examples
    
    Args:
        description: M√¥ t·∫£ bug
        model: "Llama" ho·∫∑c "GPT-5"
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"üîç CLASSIFY_BUG - Model: {model}")
    logger.info(f"üìù Input: {description[:100]}..." if len(description) > 100 else f"üìù Input: {description}")
    
    # B∆∞·ªõc 1: Th·ª≠ keyword heuristic (nhanh nh·∫•t)
    heuristic_result = _quick_heuristic_for_text(description)
    if heuristic_result:
        logger.info(f"‚ö° Heuristic match: {heuristic_result}")
        # Kh√¥ng l∆∞u v√†o ChromaDB - qu√° r√µ r√†ng, ch·ªâ d·ª±a v√†o keywords
        return heuristic_result
    
    # X√°c ƒë·ªãnh embedding type d·ª±a tr√™n model
    use_local = (model == "Llama")
    
    # B∆∞·ªõc 2: Semantic search trong ChromaDB (bugs ƒë√£ classify)
    if CHROMA_AVAILABLE:
        try:
            similar_bugs = search_similar_classified_bugs(
                query=description,
                top_k=1,
                similarity_threshold=0.85,  # High similarity threshold (85%)
                use_local_embeddings=use_local
            )
            
            if similar_bugs and len(similar_bugs) > 0:
                best_match = similar_bugs[0]
                similarity = best_match.get('similarity', 0)
                
                if similarity >= 0.85:  # Very similar bug found
                    result = {
                        'label': best_match['metadata']['label'],
                        'reason': f"Similar to: '{best_match['text'][:60]}...' (semantic: {similarity:.0%})",
                        'team': best_match['metadata'].get('team'),
                        'severity': best_match['metadata'].get('severity')
                    }
                    logger.info(f"üéØ Semantic match: {result}")
                    # Kh√¥ng l∆∞u v√†o ChromaDB - ƒë√£ c√≥ bug t∆∞∆°ng t·ª± trong DB
                    return result
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ChromaDB search failed: {e}")
    
    # B∆∞·ªõc 3: Get dynamic few-shot examples t·ª´ ChromaDB
    dynamic_examples = FEW_SHOT_EXAMPLES  # Default
    if CHROMA_AVAILABLE:
        try:
            retrieved_examples = get_dynamic_few_shot_examples(
                description,
                top_k=5,
                use_local_embeddings=use_local
            )
            if retrieved_examples:
                # Convert to format compatible v·ªõi existing code
                dynamic_examples = [
                    {'description': ex['description'], 'label': ex['label']}
                    for ex in retrieved_examples
                ]
                logger.info(f"‚úÖ Using {len(dynamic_examples)} dynamic examples")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get dynamic examples: {e}")
    
    # B∆∞·ªõc 4: LLM classification theo model ƒë∆∞·ª£c ch·ªçn
    if model == "Llama":
        # X·ª≠ l√Ω LLAMA
        if not LLAMA_AVAILABLE:
            logger.error(f"‚ùå Llama kh√¥ng kh·∫£ d·ª•ng")
            return {"label": "", "reason": "Llama model not available", "team": None, "severity": None}
        
        try:
            logger.info("ü¶ô ƒêang x·ª≠ l√Ω b·∫±ng Llama...")
            llama_service = get_llama_service()
            result = await asyncio.to_thread(
                llama_service.classify_bug,
                description,
                list(BUG_LABELS.keys()),
                dynamic_examples  # Use dynamic examples
            )
            # Map team
            if not result.get('team') and result.get('label'):
                result['team'] = LABEL_TO_TEAM.get(result['label'])
            logger.info(f"‚úÖ Llama result: {result}")
            
            # L∆∞u v√†o ChromaDB ƒë·ªÉ h·ªçc t·ª´ classification n√†y (d√πng local embeddings cho Llama)
            if CHROMA_AVAILABLE and result.get('label'):
                try:
                    add_classified_bug_to_vector_store(
                        bug_text=description,
                        label=result['label'],
                        reason=result.get('reason', ''),
                        team=result.get('team'),
                        severity=result.get('severity'),
                        use_local_embeddings=True  # Local embeddings cho Llama
                    )
                    logger.info("üíæ Saved to vector store (LOCAL embeddings)")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to save to vector store: {e}")
            
            return result
        except Exception as e:
            logger.error(f"‚ùå Llama l·ªói: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"label": "", "reason": f"Llama error: {str(e)}", "team": None, "severity": None}
    
    elif model == "GPT-5":
        # X·ª≠ l√Ω GPT
        if not GPT_AVAILABLE:
            logger.error("‚ùå GPT kh√¥ng kh·∫£ d·ª•ng")
            return {"label": "", "reason": "GPT model not available", "team": None, "severity": None}
        
        try:
            logger.info("ü§ñ ƒêang x·ª≠ l√Ω b·∫±ng GPT...")
            gpt_service = get_gpt_service()
            result = await gpt_service.classify_bug(
                description=description,
                labels=list(BUG_LABELS.keys()),
                label_descriptions=label_descriptions,
                example_text=example_text,
                team_groups=list(TEAM_GROUPS.keys())
            )
            # Map team
            if not result.get('team') and result.get('label'):
                result['team'] = LABEL_TO_TEAM.get(result['label'])
            logger.info(f"‚úÖ GPT result: {result}")
            
            # L∆∞u v√†o ChromaDB ƒë·ªÉ h·ªçc t·ª´ classification n√†y (d√πng OpenAI embeddings cho GPT)
            if CHROMA_AVAILABLE and result.get('label'):
                try:
                    add_classified_bug_to_vector_store(
                        bug_text=description,
                        label=result['label'],
                        reason=result.get('reason', ''),
                        team=result.get('team'),
                        severity=result.get('severity'),
                        use_local_embeddings=False  # OpenAI embeddings cho GPT
                    )
                    logger.info("üíæ Saved to vector store (OPENAI embeddings)")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to save to vector store: {e}")
            
            return result
        except Exception as e:
            logger.error(f"‚ùå GPT l·ªói: {e}")
            return {"label": "", "reason": f"GPT error: {str(e)}", "team": None, "severity": None}
    
    else:
        # Model kh√¥ng h·ªó tr·ª£
        logger.error(f"‚ùå Model '{model}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£")
        return {"label": "", "reason": f"Unsupported model: {model}", "team": None, "severity": None}


async def batch_classify(descriptions: List[str], model: str = "GPT-5"):
    logger.info(f"\n{'='*80}")
    logger.info(f"üì¶ BATCH_CLASSIFY - Model: {model}, Count: {len(descriptions)}")
    results: List[Optional[dict]] = [None] * len(descriptions)

    # X√°c ƒë·ªãnh embedding type
    use_local = (model == "Llama")
    
    # TIER 1: Heuristic pass (keyword matching >60%)
    remaining_indexes = []
    for i, desc in enumerate(descriptions):
        h = _quick_heuristic_for_text(desc)
        if h:
            results[i] = h
        else:
            remaining_indexes.append(i)
    
    logger.info(f"‚ö° Tier 1 Heuristic: {len(descriptions) - len(remaining_indexes)}/{len(descriptions)} matched")

    if not remaining_indexes:
        # Heuristic match 100% ‚Üí Kh√¥ng c·∫ßn l∆∞u v√†o ChromaDB (qu√° r√µ r√†ng)
        return results
    
    # TIER 2: ChromaDB Semantic Search (>85% similarity)
    semantic_remaining = []
    if CHROMA_AVAILABLE:
        logger.info(f"üîç Tier 2 Semantic Search: Checking {len(remaining_indexes)} bugs...")
        for idx in remaining_indexes:
            try:
                query_text = descriptions[idx]
                logger.debug(f"Searching for bug {idx}: {query_text[:100]}...")
                
                similar_bugs = search_similar_classified_bugs(
                    query=query_text,
                    top_k=3,  # L·∫•y 3 k·∫øt qu·∫£ ƒë·ªÉ debug
                    similarity_threshold=0.0,  # T·∫Øt threshold ƒë·ªÉ th·∫•y t·∫•t c·∫£
                    use_local_embeddings=use_local
                )
                
                if similar_bugs and len(similar_bugs) > 0:
                    best_match = similar_bugs[0]
                    similarity = best_match.get('similarity', 0)
                    
                    # Debug: In ra top 3 matches
                    if len(similar_bugs) > 1:
                        logger.info(f"   Bug {idx} top matches: " + ", ".join([f"{s.get('similarity', 0):.1%}" for s in similar_bugs[:3]]))
                    
                    if similarity >= 0.85:
                        results[idx] = {
                            'label': best_match['metadata']['label'],
                            'reason': f"Similar: '{best_match['text'][:40]}...' ({similarity:.0%})",
                            'team': best_match['metadata'].get('team'),
                            'severity': best_match['metadata'].get('severity')
                        }
                        continue
                    else:
                        # Similarity < 85% ‚Üí Kh√¥ng ƒë·ªß tin c·∫≠y, c·∫ßn LLM
                        logger.info(f"   Bug {idx}: Found similar but only {similarity:.1%} < 85% threshold")
            except Exception as e:
                logger.error(f"‚ùå Semantic search failed for bug {idx}: {e}", exc_info=True)
            
            semantic_remaining.append(idx)
        
        semantic_matched = len(remaining_indexes) - len(semantic_remaining)
        logger.info(f"‚úÖ Tier 2 Semantic: {semantic_matched}/{len(remaining_indexes)} matched")
        remaining_indexes = semantic_remaining
    
    if not remaining_indexes:
        # Semantic match t·ª´ ChromaDB ‚Üí Kh√¥ng c·∫ßn l∆∞u l·∫°i (ƒë√£ c√≥ trong DB)
        return results
    
    # TIER 3 & 4: LLM Classification v·ªõi dynamic few-shot examples
    logger.info(f"ü§ñ Tier 3+4 LLM: Processing {len(remaining_indexes)} bugs with {model}...")
    if model == "Llama":
        # X·ª≠ l√Ω LLAMA batch
        if not LLAMA_AVAILABLE:
            logger.error(f"‚ùå Llama kh√¥ng kh·∫£ d·ª•ng")
            for idx in remaining_indexes:
                results[idx] = {"label": "", "reason": "Llama model not available", "team": None, "severity": None}
            return results
        
        logger.info("ü¶ô ƒêang x·ª≠ l√Ω batch b·∫±ng Llama...")
        llama_service = get_llama_service()
        
        # Get descriptions for remaining bugs
        remaining_descriptions = [descriptions[idx] for idx in remaining_indexes]
        
        # Try batch classification first (more efficient)
        try:
            batch_results = await asyncio.to_thread(
                llama_service.batch_classify_bugs,
                remaining_descriptions,
                list(BUG_LABELS.keys()),
                FEW_SHOT_EXAMPLES
            )
            
            # Map results back
            for i, idx in enumerate(remaining_indexes):
                if i < len(batch_results):
                    result = batch_results[i]
                    # Add team mapping
                    if not result.get('team') and result.get('label'):
                        result['team'] = LABEL_TO_TEAM.get(result['label'])
                    results[idx] = result
            
            logger.info(f"‚úÖ Llama batch classification complete")
        except Exception as e:
            logger.error(f"‚ùå Llama batch error: {e}, falling back to individual classification")
            # Fallback: classify individually
            for idx in remaining_indexes:
                try:
                    result = await classify_bug(descriptions[idx], model="Llama")
                    results[idx] = result
                except Exception as e2:
                    logger.error(f"‚ùå Llama l·ªói bug {idx}: {e2}")
                    results[idx] = {"label": "", "reason": f"Llama error: {str(e2)}", "team": None, "severity": None}
        
    
    elif model == "GPT-5":
        # X·ª≠ l√Ω GPT batch
        if not GPT_AVAILABLE:
            logger.error("‚ùå GPT kh√¥ng kh·∫£ d·ª•ng")
            for idx in remaining_indexes:
                results[idx] = {"label": "", "reason": "GPT model not available", "team": None, "severity": None}
            return results
        
        logger.info("ü§ñ ƒêang x·ª≠ l√Ω batch b·∫±ng GPT...")
        gpt_service = get_gpt_service()
        
        # L·∫•y descriptions v√† indexes c√≤n l·∫°i
        remaining_descriptions = [descriptions[idx] for idx in remaining_indexes]
        
        # G·ªçi GPT batch API
        batch_results = await gpt_service.batch_classify(
            descriptions=remaining_descriptions,
            indexes=remaining_indexes,
            labels=list(BUG_LABELS.keys()),
            label_descriptions=label_descriptions,
            example_text=example_text,
            team_groups=list(TEAM_GROUPS.keys())
        )
        
        # Map k·∫øt qu·∫£ v·ªÅ
        for idx, result in batch_results.items():
            if 0 <= idx < len(results):
                if not result.get('team') and result.get('label'):
                    result['team'] = LABEL_TO_TEAM.get(result['label'])
                results[idx] = result
    
    else:
        # Model kh√¥ng h·ªó tr·ª£
        logger.error(f"‚ùå Model '{model}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£")
        for idx in remaining_indexes:
            results[idx] = {"label": "", "reason": f"Unsupported model: {model}", "team": None, "severity": None}
        return results

    # Fallback individual classification for None entries
    none_count = sum(1 for r in results if r is None)
    if none_count > 0:
        logger.info(f"üîÑ Fallback individual classification for {none_count} bugs")
    
    for i in range(len(results)):
        if results[i] is None:
            try:
                results[i] = await classify_bug(descriptions[i], model=model)
            except Exception as e:
                logger.error(f"‚ùå Failed to classify bug {i}: {e}")
                results[i] = {
                    "label": "",
                    "reason": "classification_failed",
                    "team": None,
                }
    
    # TIER 5: L∆∞u t·∫•t c·∫£ k·∫øt qu·∫£ v√†o ChromaDB ƒë·ªÉ h·ªçc v√† c·∫£i thi·ªán
    if CHROMA_AVAILABLE:
        saved_count = 0
        for i, result in enumerate(results):
            if result and result.get('label'):
                try:
                    add_classified_bug_to_vector_store(
                        bug_text=descriptions[i],
                        label=result['label'],
                        reason=result.get('reason', ''),
                        team=result.get('team'),
                        severity=result.get('severity'),
                        use_local_embeddings=use_local
                    )
                    saved_count += 1
                except Exception as e:
                    logger.debug(f"Failed to save bug {i}: {e}")
        
        logger.info(f"üíæ Saved {saved_count}/{len(results)} results to ChromaDB")
    
    logger.info(f"‚úÖ Batch classification complete: {len(results)} results")
    logger.info(f"{'='*80}\n")
    return results


# CLI interface khi ch·∫°y tr·ª±c ti·∫øp
if __name__ == "__main__":
    bug_report = input("Nh·∫≠p n·ªôi dung bug report: ")

    try:
        res = asyncio.run(classify_bug(bug_report))
    except Exception as e:
        print(f"Classification error: {e}")
        res = None

    if isinstance(res, dict):
        print(
            f"\nBug report: {bug_report}\nPh√¢n lo·∫°i: {res.get('label')}\nL√Ω do: {res.get('reason')}"
        )
    else:
        print(f"\nBug report: {bug_report}\nPh√¢n lo·∫°i: {res}")
    input(".")
